# Football Match Prediction â€” MLOps Pipeline

This repository houses a **production-ready MLOps pipeline** for predicting football match outcomes across multiple leagues and seasons.  
It automates the entire lifecycle: **data ingestion, preprocessing, model training, drift monitoring, conditional retraining, champion selection, API serving, and dashboard visualisation**.

Everything is:
- versioned with **DVC**
- logged with **MLflow**
- containerised with **Docker**
- deployed on **Google Cloud Run**

---

##  Useful Links

- ** MLflow Experiments (DagsHub)**  
  https://dagshub.com/LEBARMS/MLOps/experiments

- ** Live API (Swagger / OpenAPI docs)**  
  https://football-mlops-api-1089778705681.europe-west4.run.app/docs#/default/predict_model2_predict_model2_post

---

##  Table of Contents

- Features  
- Pipeline Overview  
- Quick Start  
- API Usage  
- Monitoring & Retraining  
- CI/CD & Deployment  
- Project Structure  
- Acknowledgements & Citation  

---

## âœ¨ Features

### Data Ingestion
Automated downloading and scraping of match fixtures and statistics via Python scripts  
(e.g. `fetch_data_universal.py`, `extract_matches_model2.py`).

### Preprocessing & Feature Engineering
Raw data is harmonised into **three modeling modes**:

- **Model 1** â€“ Goal regression (XGBoost Regressor)
- **Model 2** â€“ Match outcome classification (Home / Draw / Away) using team strength and xG
- **Model 3** â€“ *Player mode* using average player strength scores for custom lineups

### Training & Evaluation
- Models trained with **XGBoost**
- Metrics logged to **MLflow**
- Evaluation reports exported to JSON and stored in `reports/`

### Champion Selection
`select_champion.py` compares models using:
1. `f1_macro`
2. `accuracy`

The selected model is written to:
```text
app/champion_config.json
Data Drift Monitoring
monitor_drift.py applies Kolmogorovâ€“Smirnov tests

Drift computed on matches, player strengths, and team statistics

If drift exceeds 30%, retraining is triggered automatically via retrain_if_drift.py

API & Dashboard
FastAPI service exposes prediction endpoints and champion info

Streamlit dashboard visualises metrics, drift status, and allows interactive inference

CI/CD
GitHub Actions workflows for scheduled retraining and deployment

Fully automated build & deploy to Google Cloud Run

ğŸ” Pipeline Overview
haskell
Copy code
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data fetch  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   Raw data (DVC)
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚Preprocessingâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      â”‚              â”‚
â–¼      â–¼              â–¼
Model1 Model2        Model3
â”‚      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
     select_champion.py
              â”‚
     champion_config.json
              â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚  API    â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
              â”‚
        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚ Dashboard â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚
      monitor_drift.py
              â”‚
      retrain_if_drift.py
ğŸš€ Quick Start
Prerequisites
Python 3.11

DVC with GCS support

bash
Copy code
pip install "dvc[gs]"
MLflow (already in requirements.txt)

Google Cloud project + service account key stored as
GCP_SERVICE_ACCOUNT_KEY (GitHub secret)

Local Setup
Clone & install
bash
Copy code
git clone https://github.com/TSM-MachLedata/MLOps.git
cd MLOps
pip install -r requirements.txt
pip install "dvc[gs]"
Configure DVC & pull artifacts
bash
Copy code
echo "$GCP_SERVICE_ACCOUNT_KEY" > gcp-key.json
dvc remote modify --local gcsremote credentialpath gcp-key.json
dvc config cache.type copy --local
dvc pull
Reproduce the pipeline
bash
Copy code
dvc repro
Run API locally
bash
Copy code
uvicorn app.main:app --host 0.0.0.0 --port 8000
Swagger UI:

bash
Copy code
http://localhost:8000/docs
Launch dashboard
bash
Copy code
streamlit run dashboard_streamlit.py
ğŸ”Œ API Usage
Predict match outcome (Model 2)
POST /predict/model2

json
Copy code
{
  "home_team": "Arsenal",
  "away_team": "Manchester City"
}
Response:

json
Copy code
{
  "model": "model2 (champion)",
  "home_team": "Arsenal",
  "away_team": "Manchester City",
  "prediction": "DRAW",
  "proba_away_win": 0.30,
  "proba_draw": 0.45,
  "proba_home_win": 0.25
}
Predict using players (Model 3)
POST /predict/model3

âš ï¸ Player names must exactly match
data/processed/player_strengths.csv

ğŸ“Š Monitoring & Retraining
Drift computed per feature and dataset

Threshold: DRIFT_THRESHOLD = 0.30

If exceeded:

Models retrained

Champion re-selected

Artifacts pushed back to DVC

Metrics logged to MLflow (DagsHub)

This ensures long-term robustness against data distribution shifts.

ğŸ”„ CI/CD & Deployment
Workflow	Purpose	Trigger
retrain.yml	Drift monitoring + conditional retraining	Daily (03:00 CET) or manual
deploy.yml	Docker build + deploy to Cloud Run	Push to main

Secrets:

GCP_SERVICE_ACCOUNT_KEY

MLflow / DagsHub credentials

ğŸ“ Project Structure
stylus
Copy code
MLOps/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ champion_config.json
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”œâ”€â”€ models/
â”œâ”€â”€ reports/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ monitor_drift.py
â”‚   â”œâ”€â”€ retrain_if_drift.py
â”‚   â”œâ”€â”€ select_champion.py
â”‚   â””â”€â”€ train*.py
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ retrain.yml
â”‚   â””â”€â”€ deploy.yml
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ“š Acknowledgements & Citation
This project accompanies the report
Â« Pipeline MLOps de prÃ©diction de matchs de football Â».

If you reuse this repository or its ideas, please consider citing or linking back.
Issues and pull requests are welcome â€” this repo is intended as a robust MLOps template for sports analytics.

livecodeserver
Copy code
